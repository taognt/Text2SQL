{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"xlangai/spider\", split='train')\n",
    "validation_dataset = load_dataset(\"xlangai/spider\", split='validation')\n",
    "db_schema = load_dataset(\"richardr1126/spider-schema\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value', 'question_toks'],\n",
      "    num_rows: 7000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value', 'question_toks'],\n",
      "    num_rows: 1034\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'Schema (values (type))', 'Primary Keys', 'Foreign Keys'],\n",
      "    num_rows: 166\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'department_management',\n",
       " 'query': 'SELECT count(*) FROM head WHERE age  >  56',\n",
       " 'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'query_toks': ['SELECT',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'head',\n",
       "  'WHERE',\n",
       "  'age',\n",
       "  '>',\n",
       "  '56'],\n",
       " 'query_toks_no_value': ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'from',\n",
       "  'head',\n",
       "  'where',\n",
       "  'age',\n",
       "  '>',\n",
       "  'value'],\n",
       " 'question_toks': ['How',\n",
       "  'many',\n",
       "  'heads',\n",
       "  'of',\n",
       "  'the',\n",
       "  'departments',\n",
       "  'are',\n",
       "  'older',\n",
       "  'than',\n",
       "  '56',\n",
       "  '?']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'Schema (values (type))': 'department : Department_ID (number) , Name (text) , Creation (text) , Ranking (number) , Budget_in_Billions (number) , Num_Employees (number) | head : head_ID (number) , name (text) , born_state (text) , age (number) | management : department_ID (number) , head_ID (number) , temporary_acting (text)', 'Primary Keys': 'department : Department_ID | head : head_ID | management : department_ID', 'Foreign Keys': 'management : head_ID equals head : head_ID | management : department_ID equals department : Department_ID'}\n"
     ]
    }
   ],
   "source": [
    "filtered_data = db_schema.filter(lambda row: row['db_id'] == \"department_management\")\n",
    "print(filtered_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use llama for generation (0-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
      "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install langchain\n",
    "!pip -qq install langchain-core\n",
    "!pip -qq install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/czlwlk_50vq1xdyv651xbn280000gn/T/ipykernel_42232/2392824868.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model = \"llama3.2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a complex and debated topic that has been explored by philosophers, theologians, scientists, and individuals from various cultures and backgrounds. There is no one definitive answer to this question, as it can vary greatly depending on individual perspectives, beliefs, and values.\n",
      "\n",
      "Some possible answers to the meaning of life include:\n",
      "\n",
      "1. **Happiness**: Many people believe that the meaning of life is to seek happiness and fulfillment. This can be achieved through relationships, personal growth, and pursuing one's passions.\n",
      "2. **Self-actualization**: According to psychologist Abraham Maslow, the meaning of life is to realize one's full potential and become the best version of oneself.\n",
      "3. **Love and connection**: For some, the meaning of life is found in loving and being loved by others, forming meaningful connections with family, friends, and community.\n",
      "4. **Personal growth and self-improvement**: Some believe that the meaning of life is to continually learn, grow, and improve oneself, both intellectually and spiritually.\n",
      "5. **Leaving a legacy**: Others see the meaning of life as leaving a lasting impact on the world, whether through creative work, scientific discoveries, or contributions to society.\n",
      "6. **Spirituality and faith**: For many people, the meaning of life is found in spiritual or religious beliefs, which provide a sense of purpose and connection to something greater than oneself.\n",
      "7. **Existentialism**: Some philosophers argue that life has no inherent meaning, and it's up to each individual to create their own purpose and meaning through choices and actions.\n",
      "8. **The pursuit of knowledge**: The desire for knowledge and understanding is another possible answer to the meaning of life. This can involve scientific inquiry, artistic expression, or philosophical exploration.\n",
      "9. **Service to others**: Some people believe that the meaning of life is found in serving others, whether through volunteering, community work, or simply being a good friend and neighbor.\n",
      "10. **The search for transcendence**: Finally, some individuals see the meaning of life as seeking experiences that transcend the ordinary, such as spiritual enlightenment, artistic expression, or a sense of oneness with the universe.\n",
      "\n",
      "Ultimately, the meaning of life is a deeply personal and subjective question that each individual must answer for themselves. What gives your life meaning and purpose may be different from what gives someone else's life meaning and purpose.\n",
      "\n",
      "It's also worth noting that there are many philosophical perspectives on the meaning of life, such as:\n",
      "\n",
      "* **Stoicism**: Emphasizes living in accordance with reason and nature.\n",
      "* **Eudaimonism**: Focuses on achieving happiness through virtuous living.\n",
      "* **Existentialism**: Highlights individual freedom and responsibility to create one's own meaning.\n",
      "* **Nihilism**: Argues that life has no inherent meaning, and it's up to each individual to create their own purpose.\n",
      "\n",
      "Each of these perspectives offers a unique perspective on the meaning of life, and individuals may find aspects of multiple philosophies resonating with them.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model = \"llama3.2\")\n",
    "answer = llm.invoke(\"what is the Meaning of life\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to generate some SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------\n",
      "\n",
      "question: What is the average number of employees of the departments whose rank is between 10 and 15?\n",
      "\n",
      "Answer: SELECT COUNT(*) FROM employees WHERE age > 56 AND job_title IN ('CEO', 'Director', 'Manager')\n",
      "\n",
      "Correct answer: SELECT count(*) FROM head WHERE age  >  56\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "Query 1 is counting the number of people with an age greater than 56 in the \"head\" table, which may not be directly comparable to Query 2 because it only considers one table (\"employees\"). Additionally, Query 2 filters by job title, whereas Query 1 does not have any such filter.\n",
      "\n",
      "--------\n",
      "\n",
      "question: How many heads of the departments are older than 56 ?\n",
      "\n",
      "Answer: SELECT name, birth_state, DATEDIFF(YEAR, birth_date, getdate()) as age FROM department_heads ORDER BY age\n",
      "\n",
      "Correct answer: SELECT name ,  born_state ,  age FROM head ORDER BY age\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "In Query 1, `age` is not included in the `SELECT` clause. In SQL Server, when you use an aggregate function like `MAX()` or `MIN()`, you need to include it in the column list. However, since we're ordering by a specific column (`age`) and not aggregating anything on that column directly, this query seems fine as is.\n",
      "\n",
      "In Query 2, the use of `DATEDIFF` with fixed start and end dates (in this case, the current date) will give you an incorrect result because it doesn't take into account the actual age at the time of insertion. It assumes all records were inserted on the same day, which is not true.\n",
      "\n",
      "In Query 1, SQL Server can infer that `age` is what's being ordered by (since it's in the `ORDER BY` clause), so you don't need to explicitly include it in the `SELECT` list if it's part of a compound index.\n",
      "\n",
      "Query 2 should be:\n",
      "```sql\n",
      "SELECT name, birth_state, DATEDIFF(YEAR, birth_date, GETDATE()) AS age FROM department_heads ORDER BY age;\n",
      "```\n",
      "Even then, this query still won't give you the same result as Query 1 because it will always calculate the difference between the `birth_date` and the current date (`GETDATE()`), regardless of when each record was actually inserted.\n",
      "\n",
      "--------\n",
      "\n",
      "question: List the name, born state and age of the heads of departments ordered by age.\n",
      "\n",
      "Answer: SELECT creation_year, name, budget FROM departments\n",
      "\n",
      "Correct answer: SELECT creation ,  name ,  budget_in_billions FROM department\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "The main difference is that in Query 1, the table name is \"department\", but in Query 2, it's spelled as \"departments\". Since the table names are different, the queries will not give the same result.\n",
      "\n",
      "--------\n",
      "\n",
      "question: List the creation year, name and budget of each department.\n",
      "\n",
      "Answer: SELECT max(budget), min(budget) FROM departments\n",
      "\n",
      "Correct answer: SELECT max(budget_in_billions) ,  min(budget_in_billions) FROM department\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "The reason for this is that the data types in SQL are not always identical. \n",
      "\n",
      "In Query 1, `budget_in_billions` is specified as a column in the `department` table. If it's stored as an integer or decimal value, then Query 1 will return integers. However, if it's stored as a string (\"x.x\" for example) because of a format issue, Query 2 will treat it as a character type and may return incorrect results.\n",
      "\n",
      "Additionally, the column names in the two queries are not identical (one uses plural form while the other doesn't), so even if the data types were consistent, they would still be considered different queries.\n",
      "\n",
      "--------\n",
      "\n",
      "question: What are the maximum and minimum budget of the departments?\n",
      "\n",
      "Answer: SELECT AVG(e.num_employees) FROM departments d JOIN employees e ON d.dept_no = e.dept_no WHERE d.rank BETWEEN 10 AND 15;\n",
      "\n",
      "Correct answer: SELECT avg(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\n",
      "\n",
      "Correct: No. \n",
      "\n",
      "Query 1 and Query 2 are using different table aliases. In the first query, \"d\" is used for the \"department\" table, while in the second query, \"d\" is not explicitly defined but instead \"e\" is used which stands for \"employees\".\n"
     ]
    }
   ],
   "source": [
    "nb_queries = 5\n",
    "prompt = \"Write the SQL query that answer the user's question. Answer only the SQL query. Question: {question}.\\nSQL Query:\"\n",
    "classification_prompt = \"Tell if these two SQL queries are giving the same result, answer yes or no only. If no, explain. Query 1: {query1}.\\nQuery 2: {query2}.\\nSame (correction if necessary):\"\n",
    "\n",
    "for i in range(nb_queries):\n",
    "    print(f\"\\n--------\\n\")\n",
    "    print(f\"question: {question}\")\n",
    "    query1 = train_dataset[i]['query']\n",
    "    question = train_dataset[i]['question']\n",
    "    prompt_completed = prompt.format(question=question)\n",
    "    query2 = llm.invoke(prompt_completed)\n",
    "    print(f\"\\nAnswer: {query2}\\n\")\n",
    "    print(f\"Correct answer: {train_dataset[i]['query']}\\n\")\n",
    "\n",
    "    correct = llm.invoke(classification_prompt.format(query1 = query1, query2 = query2))\n",
    "    print(f\"Correct: {correct}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 166/166 [00:00<00:00, 27843.50 examples/s]]\n",
      "Filter: 100%|██████████| 166/166 [00:00<00:00, 27048.46 examples/s]]\n",
      "Processing queries: 100%|██████████| 100/100 [01:25<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = r'\\b(yes|no)\\b'\n",
    "nb_queries = 100\n",
    "prompt_schema = \"Based on the SQL schema, write a SQL query that answer the user's question. Answer only the SQL query. Schema: {schema}.\\nQuestion: {question}.\\nSQL Query:\"\n",
    "classification_prompt = \"Tell if these two SQL queries are giving the same result, answer yes or no only. Query 1: {query1}.\\nQuery 2: {query2}.\\nSame (correction if necessary):\"\n",
    "verbose = False\n",
    "nb_correct = 0\n",
    "list_incorrect = []\n",
    "\n",
    "for i in tqdm(range(nb_queries), desc=\"Processing queries\"):\n",
    "    dataset_i = train_dataset[i]\n",
    "    db_id = dataset_i['db_id']\n",
    "    filtered_data = db_schema.filter(lambda row: row['db_id'] == db_id)\n",
    "    schema = filtered_data[0]\n",
    "    \n",
    "    query1 = dataset_i['query']\n",
    "    question = dataset_i['question']\n",
    "    prompt_completed = prompt.format(question=question, schema=schema)\n",
    "    query2 = llm.invoke(prompt_completed)\n",
    "    correct = llm.invoke(classification_prompt.format(query1 = query1, query2 = query2))\n",
    "    matches = re.findall(pattern, correct, flags=re.IGNORECASE)\n",
    "\n",
    "    # Increment nb_yes for each \"yes\" found\n",
    "    nb_correct += sum(1 for match in matches if match.lower() == 'yes')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--------\")\n",
    "        print(f\"question: {question}\")\n",
    "        print(f\"schema: {schema}\")\n",
    "        print(f\"\\nAnswer: {query2}\\n\")\n",
    "        print(f\"Correct answer: {dataset_i['query']}\")\n",
    "        print(f\"Correct: {correct}\")\n",
    "\n",
    "print(f\"Accuracy: {nb_correct/nb_queries}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'department_management',\n",
       " 'query': 'SELECT count(*) FROM head WHERE age  >  56',\n",
       " 'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'query_toks': ['SELECT',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'head',\n",
       "  'WHERE',\n",
       "  'age',\n",
       "  '>',\n",
       "  '56'],\n",
       " 'query_toks_no_value': ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'from',\n",
       "  'head',\n",
       "  'where',\n",
       "  'age',\n",
       "  '>',\n",
       "  'value'],\n",
       " 'question_toks': ['How',\n",
       "  'many',\n",
       "  'heads',\n",
       "  'of',\n",
       "  'the',\n",
       "  'departments',\n",
       "  'are',\n",
       "  'older',\n",
       "  'than',\n",
       "  '56',\n",
       "  '?']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
