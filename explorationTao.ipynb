{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taoguinot/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"xlangai/spider\", split='train')\n",
    "validation_dataset = load_dataset(\"xlangai/spider\", split='validation')\n",
    "db_schema = load_dataset(\"richardr1126/spider-schema\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'Schema (values (type))': 'department : Department_ID (number) , Name (text) , Creation (text) , Ranking (number) , Budget_in_Billions (number) , Num_Employees (number) | head : head_ID (number) , name (text) , born_state (text) , age (number) | management : department_ID (number) , head_ID (number) , temporary_acting (text)', 'Primary Keys': 'department : Department_ID | head : head_ID | management : department_ID', 'Foreign Keys': 'management : head_ID equals head : head_ID | management : department_ID equals department : Department_ID'}\n"
     ]
    }
   ],
   "source": [
    "filtered_data = db_schema.filter(lambda row: row['db_id'] == \"department_management\")\n",
    "print(filtered_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use llama for generation (0-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install langchain\n",
    "!pip -qq install langchain-core\n",
    "!pip -qq install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/czlwlk_50vq1xdyv651xbn280000gn/T/ipykernel_52119/2581367007.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = LLM(model=Ollama(model=\"llama3.2\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from llm import LLM\n",
    "\n",
    "llm = LLM(model=Ollama(model=\"llama3.2\"))\n",
    "llm.model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to generate some SQL queries (0-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_queries = 5\n",
    "prompt = \"Write the SQL query that answer the user's question. Answer only the SQL query, write SQL operators (COUNT, AVG, etc.) in MAJ. Question: {question}.\\nSQL Query:\"\n",
    "classification_prompt = \"Tell if these two SQL queries are giving the same result, answer yes or no only. If no, explain. Query 1: {query1}.\\nQuery 2: {query2}.\\nSame (correction if necessary):\"\n",
    "\n",
    "for i in range(nb_queries):\n",
    "    dataset_i = train_dataset[i]\n",
    "    question = dataset_i[\"question\"]\n",
    "    print(f\"\\n--------\\n\")\n",
    "    print(f\"question: {question}\")\n",
    "    query1 = dataset_i['query']\n",
    "    prompt_completed = prompt.format(question=question)\n",
    "    query2 = llm.invoke(prompt_completed)\n",
    "    print(f\"\\nAnswer: {query2}\\n\")\n",
    "    print(f\"Correct answer: {dataset_i['query']}\\n\")\n",
    "\n",
    "    correct = llm.invoke(classification_prompt.format(query1 = query1, query2 = query2))\n",
    "    print(f\"Correct: {correct}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import classification_prompt, prompt_schema\n",
    "\n",
    "def generate_query(question, llm, prompt=prompt_schema, schema=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    prompt_completed = prompt.format(question=question, schema=schema)\n",
    "    generated_query = llm.invoke(prompt_completed)\n",
    "\n",
    "    return generated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "score_prompt = \"\"\"Determine the degree of logical equivalence between the two SQL queries, assuming the same schema, data, and execution environment. Provide a score between 0 and 1, where:\n",
    "\n",
    "- 1: Fully logically equivalent (queries produce identical results under all circumstances).\n",
    "- 0: Completely different (queries are logically unrelated or produce entirely different results).\n",
    "- Scores between 0 and 1 should reflect partial equivalence, considering factors such as:\n",
    "  - Differences in filters, conditions, or joins that partially overlap.\n",
    "  - Minor variations in selected columns or formatting that do not affect the overall logic.\n",
    "  - Similar intent but differing specifics in query structure.\n",
    "\n",
    "Explain your score briefly, highlighting key differences or similarities that influenced the rating.\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Generated query:\n",
    "{generated_query}\n",
    "\n",
    "Equivalence Score (0-1, with explanation):\"\"\"\n",
    "\n",
    "def equivalence_score(generated_query, query, llm, score_prompt=score_prompt) -> bool:\n",
    "    \"\"\"\n",
    "    Return an equivalence score between generated query and a groundtruth query with an orchestrator LLM\n",
    "    \"\"\"\n",
    "    pattern = r\"\\s*([0-9]*\\.?[0-9]+)\"\n",
    "    explanation = llm.invoke(score_prompt.format(query = query, generated_query = generated_query))\n",
    "    match = re.search(pattern, explanation, flags=re.IGNORECASE)\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "    \n",
    "    return score, explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(generated_query, query, llm, classification_prompt=classification_prompt) -> bool:\n",
    "    \"\"\"\n",
    "    Return true if generated query is considered as equivalent to query in terms of result by an orchestrator LLM\n",
    "    \"\"\"\n",
    "    pattern = r'\\b(yes|no)\\b'\n",
    "    correct = llm.invoke(classification_prompt.format(query = query, generated_query = generated_query))\n",
    "    matches = re.findall(pattern, correct, flags=re.IGNORECASE)\n",
    "\n",
    "    return \"Yes\" in matches or \"yes\" in matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 5/5 [00:11<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from parameters import classification_prompt, prompt_schema\n",
    "\n",
    "# pattern = r'\\b(yes|no)\\b'\n",
    "nb_queries = 5\n",
    "\n",
    "verbose = False\n",
    "nb_correct = 0\n",
    "list_incorrect = []\n",
    "\n",
    "for i in tqdm(range(nb_queries), desc=\"Processing queries\"):\n",
    "    dataset_i = train_dataset[i]\n",
    "    db_id = dataset_i['db_id']\n",
    "    filtered_data = db_schema.filter(lambda row: row['db_id'] == db_id)\n",
    "    schema = filtered_data[0]\n",
    "    \n",
    "    query = dataset_i['query']\n",
    "    question = dataset_i['question']\n",
    "    \n",
    "    generated_query = llm.generate_query(question, prompt_schema, schema)\n",
    "    correct = llm.is_correct(generated_query, query, classification_prompt)\n",
    "\n",
    "    # Increment nb_yes for each \"yes\" found\n",
    "    if correct:\n",
    "        nb_correct += 1\n",
    "\n",
    "    if not correct:\n",
    "        list_incorrect.append({\n",
    "            'llm_answer':generated_query,\n",
    "            'correct_answer':query,\n",
    "            'classification':correct\n",
    "        })\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--------\")\n",
    "        print(f\"question: {question}\")\n",
    "        print(f\"schema: {schema}\")\n",
    "        print(f\"\\nAnswer: {generated_query}\\n\")\n",
    "        print(f\"Correct answer: {dataset_i['query']}\")\n",
    "        print(f\"Correct: {correct}\")\n",
    "\n",
    "print(f\"Accuracy: {nb_correct/nb_queries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_incorrect)):\n",
    "    print(\"\\n\")\n",
    "    print(f\"LLM answer: {list_incorrect[i]['llm_answer']}\")\n",
    "    print(f\"Correct answer: {list_incorrect[i]['correct_answer']}\")\n",
    "    print(f\"Correct: {list_incorrect[i]['classification']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many heads of the departments are older than 56 ?\n",
      "Query: SELECT count(*) FROM head WHERE age  >  56\n",
      "Generate_query: SELECT COUNT(*) FROM department_heads WHERE age > 56;\n"
     ]
    }
   ],
   "source": [
    "from utils import get_words_between_keywords, compute_metrics\n",
    "\n",
    "test_data = train_dataset[0]\n",
    "\n",
    "question = test_data['question']\n",
    "query = test_data['query']\n",
    "generated_query = llm.generate_query(question, prompt_schema)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Generate_query: {generated_query}\")\n",
    "\n",
    "correct = llm.is_correct(generated_query, query, classification_prompt)\n",
    "equivalence_score, explanation = llm.equivalence_score(generated_query, query)\n",
    "valid_pred, keyword_score, identifier_score = compute_metrics(generated_query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: False\n",
      "valid_pred: True\n",
      "keyword_score: 1.0\n",
      "identifier_score: 0.0\n",
      "Equivalence score: 0.5\n",
      "Explanation: I would assign an equivalence score of 0.5 to these two SQL queries.\n",
      "\n",
      "The primary reason for this score is that the generated query (Query 2) has a different table name (`department_heads`) compared to Query 1 (`head`). This difference in table name means that Query 2 will return different data than Query 1, as it only considers rows in `department_heads` with an age greater than 56.\n",
      "\n",
      "However, there are minor similarities between the two queries. Both use the same filter condition (`age > 56`) and both select the count of rows that satisfy this condition. Additionally, the queries have a similar structure, using the `SELECT COUNT(*)` syntax to count the number of rows meeting the specified criteria.\n",
      "\n",
      "Overall, while Query 2 is not fully logically equivalent to Query 1 due to the difference in table name, it shares some similarities in its logic and structure, which warrants a score of 0.5.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct: {correct}\")\n",
    "print(f\"valid_pred: {valid_pred}\")\n",
    "print(f\"keyword_score: {keyword_score}\")\n",
    "print(f\"identifier_score: {identifier_score}\")\n",
    "print(f\"Equivalence score: {equivalence_score}\")\n",
    "print(f\"Explanation: {explanation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
