{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"xlangai/spider\", split='train')\n",
    "validation_dataset = load_dataset(\"xlangai/spider\", split='validation')\n",
    "db_schema = load_dataset(\"richardr1126/spider-schema\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value', 'question_toks'],\n",
      "    num_rows: 7000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value', 'question_toks'],\n",
      "    num_rows: 1034\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'Schema (values (type))', 'Primary Keys', 'Foreign Keys'],\n",
      "    num_rows: 166\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(db_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'department_management',\n",
       " 'query': 'SELECT count(*) FROM head WHERE age  >  56',\n",
       " 'question': 'How many heads of the departments are older than 56 ?',\n",
       " 'query_toks': ['SELECT',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'FROM',\n",
       "  'head',\n",
       "  'WHERE',\n",
       "  'age',\n",
       "  '>',\n",
       "  '56'],\n",
       " 'query_toks_no_value': ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  'from',\n",
       "  'head',\n",
       "  'where',\n",
       "  'age',\n",
       "  '>',\n",
       "  'value'],\n",
       " 'question_toks': ['How',\n",
       "  'many',\n",
       "  'heads',\n",
       "  'of',\n",
       "  'the',\n",
       "  'departments',\n",
       "  'are',\n",
       "  'older',\n",
       "  'than',\n",
       "  '56',\n",
       "  '?']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'Schema (values (type))': 'department : Department_ID (number) , Name (text) , Creation (text) , Ranking (number) , Budget_in_Billions (number) , Num_Employees (number) | head : head_ID (number) , name (text) , born_state (text) , age (number) | management : department_ID (number) , head_ID (number) , temporary_acting (text)', 'Primary Keys': 'department : Department_ID | head : head_ID | management : department_ID', 'Foreign Keys': 'management : head_ID equals head : head_ID | management : department_ID equals department : Department_ID'}\n"
     ]
    }
   ],
   "source": [
    "filtered_data = db_schema.filter(lambda row: row['db_id'] == \"department_management\")\n",
    "print(filtered_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use llama for generation (0-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \n",
      "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \n",
      "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -qq install langchain\n",
    "!pip -qq install langchain-core\n",
    "!pip -qq install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/czlwlk_50vq1xdyv651xbn280000gn/T/ipykernel_53370/2581367007.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = LLM(model=Ollama(model=\"llama3.2\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from llm import LLM\n",
    "\n",
    "llm = LLM(model=Ollama(model=\"llama3.2\"))\n",
    "llm.model.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to generate some SQL queries (0-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------\n",
      "\n",
      "question: How many heads of the departments are older than 56 ?\n",
      "\n",
      "Answer: SELECT COUNT(*) FROM Employees WHERE Age > 56 AND Department = 'Head'\n",
      "\n",
      "Correct answer: SELECT count(*) FROM head WHERE age  >  56\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "The difference lies in the table names and the conditions applied to those tables. Query 1 is looking at a \"head\" table, whereas Query 2 is referencing an \"Employees\" table with a condition on the \"Department\" column being equal to 'Head'. They are likely referencing different datasets or tables in your database.\n",
      "\n",
      "--------\n",
      "\n",
      "question: List the name, born state and age of the heads of departments ordered by age.\n",
      "\n",
      "Answer: SELECT e1.name, e2.state, TIMESTAMPDIFF(YEAR, e2.born, CURDATE()) AS age FROM employees e1 JOIN employees e2 ON e1.manager = e2.emp_no GROUP BY e1.name, e2.state ORDER BY e2.born;\n",
      "\n",
      "Correct answer: SELECT name ,  born_state ,  age FROM head ORDER BY age\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "Query 1 is a simple select with an order by clause. It will return all rows in the 'head' table ordered by age.\n",
      "\n",
      "Query 2 does not return \"age\" as column name, instead it returns TIMESTAMPDIFF(YEAR, e2.born, CURDATE()) AS age which calculates how many years are between the birth year of each employee (e2) and current date.\n",
      "\n",
      "--------\n",
      "\n",
      "question: List the creation year, name and budget of each department.\n",
      "\n",
      "Answer: ```sql\n",
      "SELECT DEPARTement.CREATION_YEAR, NOM_DEP, BUDGET\n",
      "FROM DEPARTAMENTS;\n",
      "```\n",
      "\n",
      "Correct answer: SELECT creation ,  name ,  budget_in_billions FROM department\n",
      "\n",
      "Correct: No.\n",
      "\n",
      "Query 1 uses a standard SQL aliasing convention, which is to use double underscores (`__`) or single underscore and square brackets (`[]`). In the second query, Query 2, it appears that the table name `DEPARTAMENTS` contains an accent mark, whereas in Query 1, it's written without accents. This means that Query 2 may not work as expected because SQL may interpret `DEPARTAMENTS` as a different table or a view, depending on the database system being used.\n",
      "\n",
      "--------\n",
      "\n",
      "question: What are the maximum and minimum budget of the departments?\n",
      "\n",
      "Answer: SELECT MAX(budget) ,  MIN(budget) FROM departments;\n",
      "\n",
      "Correct answer: SELECT max(budget_in_billions) ,  min(budget_in_billions) FROM department\n",
      "\n",
      "Correct: Yes\n",
      "\n",
      "--------\n",
      "\n",
      "question: What is the average number of employees of the departments whose rank is between 10 and 15?\n",
      "\n",
      "Answer: SELECT AVG(Employees) FROM Departments WHERE Rank BETWEEN 10 AND 15;\n",
      "\n",
      "Correct answer: SELECT avg(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\n",
      "\n",
      "Correct: Yes.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Both queries are selecting the average number of employees from departments with a ranking between 10 and 15. The syntax may be slightly different, but they both achieve the same result.\n",
      "\n",
      "In Query 1, 'num_employees' is likely an alias for 'Employees', which in this context refers to the number of employees in each department.\n",
      "\n",
      "Query 2 uses 'AVG(Employees)' with a 'BETWEEN' condition on Rank. The use of 'Rank' instead of 'ranking' and using single quotes around 'Departments' is consistent with SQL syntax, but it's more conventional to write 'Departments' without the quotes.\n"
     ]
    }
   ],
   "source": [
    "nb_queries = 5\n",
    "prompt = \"Write the SQL query that answer the user's question. Answer only the SQL query, write SQL operators (COUNT, AVG, etc.) in MAJ. Question: {question}.\\nSQL Query:\"\n",
    "classification_prompt = \"Tell if these two SQL queries are giving the same result, answer yes or no only. If no, explain. Query 1: {query1}.\\nQuery 2: {query2}.\\nSame (correction if necessary):\"\n",
    "\n",
    "for i in range(nb_queries):\n",
    "    dataset_i = train_dataset[i]\n",
    "    question = dataset_i[\"question\"]\n",
    "    print(f\"\\n--------\\n\")\n",
    "    print(f\"question: {question}\")\n",
    "    query1 = dataset_i['query']\n",
    "    prompt_completed = prompt.format(question=question)\n",
    "    query2 = llm.model.invoke(prompt_completed)\n",
    "    print(f\"\\nAnswer: {query2}\\n\")\n",
    "    print(f\"Correct answer: {dataset_i['query']}\\n\")\n",
    "\n",
    "    correct = llm.model.invoke(classification_prompt.format(query1 = query1, query2 = query2))\n",
    "    print(f\"Correct: {correct}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import classification_prompt, prompt_schema\n",
    "\n",
    "def generate_query(question, llm, prompt=prompt_schema, schema=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    prompt_completed = prompt.format(question=question, schema=schema)\n",
    "    generated_query = llm.model.invoke(prompt_completed)\n",
    "\n",
    "    return generated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "score_prompt = \"\"\"Determine the degree of logical equivalence between the two SQL queries, assuming the same schema, data, and execution environment. Provide a score between 0 and 1, where:\n",
    "\n",
    "- 1: Fully logically equivalent (queries produce identical results under all circumstances).\n",
    "- 0: Completely different (queries are logically unrelated or produce entirely different results).\n",
    "- Scores between 0 and 1 should reflect partial equivalence, considering factors such as:\n",
    "  - Differences in filters, conditions, or joins that partially overlap.\n",
    "  - Minor variations in selected columns or formatting that do not affect the overall logic.\n",
    "  - Similar intent but differing specifics in query structure.\n",
    "\n",
    "Explain your score briefly, highlighting key differences or similarities that influenced the rating.\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Generated query:\n",
    "{generated_query}\n",
    "\n",
    "Equivalence Score (0-1, with explanation):\"\"\"\n",
    "\n",
    "def equivalence_score(generated_query, query, llm, score_prompt=score_prompt) -> bool:\n",
    "    \"\"\"\n",
    "    Return an equivalence score between generated query and a groundtruth query with an orchestrator LLM\n",
    "    \"\"\"\n",
    "    pattern = r\"\\s*([0-9]*\\.?[0-9]+)\"\n",
    "    explanation = llm.invoke(score_prompt.format(query = query, generated_query = generated_query))\n",
    "    match = re.search(pattern, explanation, flags=re.IGNORECASE)\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "    \n",
    "    return score, explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(generated_query, query, llm, classification_prompt=classification_prompt) -> bool:\n",
    "    \"\"\"\n",
    "    Return true if generated query is considered as equivalent to query in terms of result by an orchestrator LLM\n",
    "    \"\"\"\n",
    "    pattern = r'\\b(yes|no)\\b'\n",
    "    correct = llm.invoke(classification_prompt.format(query = query, generated_query = generated_query))\n",
    "    matches = re.findall(pattern, correct, flags=re.IGNORECASE)\n",
    "\n",
    "    return \"Yes\" in matches or \"yes\" in matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 10/10 [00:31<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from parameters import classification_prompt, prompt_schema\n",
    "\n",
    "# pattern = r'\\b(yes|no)\\b'\n",
    "nb_queries = 10\n",
    "\n",
    "verbose = False\n",
    "nb_correct = 0\n",
    "list_incorrect = []\n",
    "\n",
    "for i in tqdm(range(nb_queries), desc=\"Processing queries\"):\n",
    "    dataset_i = train_dataset[i]\n",
    "    db_id = dataset_i['db_id']\n",
    "    filtered_data = db_schema.filter(lambda row: row['db_id'] == db_id)\n",
    "    schema = filtered_data[0]\n",
    "    \n",
    "    query = dataset_i['query']\n",
    "    question = dataset_i['question']\n",
    "    \n",
    "    generated_query = llm.generate_query(question, prompt_schema, schema)\n",
    "    correct = llm.is_correct(generated_query, query, classification_prompt)\n",
    "\n",
    "    # Increment nb_yes for each \"yes\" found\n",
    "    if correct:\n",
    "        nb_correct += 1\n",
    "\n",
    "    if not correct:\n",
    "        list_incorrect.append({\n",
    "            'llm_answer':generated_query,\n",
    "            'correct_answer':query,\n",
    "            'classification':correct\n",
    "        })\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--------\")\n",
    "        print(f\"question: {question}\")\n",
    "        print(f\"schema: {schema}\")\n",
    "        print(f\"\\nAnswer: {generated_query}\\n\")\n",
    "        print(f\"Correct answer: {dataset_i['query']}\")\n",
    "        print(f\"Correct: {correct}\")\n",
    "\n",
    "print(f\"Accuracy: {nb_correct/nb_queries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM answer: SELECT MAX(Budget_in_Billions) ,  MIN(Budget_in_Billions) FROM department\n",
      "Correct answer: SELECT max(budget_in_billions) ,  min(budget_in_billions) FROM department\n",
      "Correct: False\n",
      "\n",
      "\n",
      "LLM answer: SELECT avg(Num_Employees) FROM department WHERE Ranking BETWEEN 10 AND 15;\n",
      "Correct answer: SELECT avg(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\n",
      "Correct: False\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_incorrect)):\n",
    "    print(\"\\n\")\n",
    "    print(f\"LLM answer: {list_incorrect[i]['llm_answer']}\")\n",
    "    print(f\"Correct answer: {list_incorrect[i]['correct_answer']}\")\n",
    "    print(f\"Correct: {list_incorrect[i]['classification']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many heads of the departments are older than 56 ?\n",
      "Query: SELECT count(*) FROM head WHERE age  >  56\n",
      "Generate_query: SELECT COUNT(*) FROM departments WHERE manager_id IN ( SELECT id FROM employees WHERE age > 56 );\n",
      "<class 'dict'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(schema))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(filtered_data))\n\u001b[0;32m---> 24\u001b[0m synth_db \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_synthetic_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(synth_db)\n\u001b[1;32m     27\u001b[0m valid_pred, keyword_score, identifier_score \u001b[38;5;241m=\u001b[39m compute_metrics(generated_query, query, test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb_id\u001b[39m\u001b[38;5;124m'\u001b[39m], synth_db)\n",
      "File \u001b[0;32m~/Documents/ENSTA/3A/LanguageModel/Text2SQLTAO/utils.py:52\u001b[0m, in \u001b[0;36mgenerate_synthetic_db\u001b[0;34m(db_schema)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_synthetic_db\u001b[39m(db_schema: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Cursor]:\n\u001b[1;32m     50\u001b[0m     sql_databases: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Cursor] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, db \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdb_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m():\n\u001b[1;32m     54\u001b[0m         conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m         sql_databases[db[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "from utils import compute_metrics, generate_synthetic_db\n",
    "from parameters import classification_prompt\n",
    "\n",
    "test_data = train_dataset[0]\n",
    "\n",
    "db_id = test_data['db_id']\n",
    "question = test_data['question']\n",
    "query = test_data['query']\n",
    "generated_query = llm.generate_query(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Generate_query: {generated_query}\")\n",
    "\n",
    "correct = llm.is_correct(generated_query, query, classification_prompt)\n",
    "equivalence_score, explanation = llm.equivalence_score(generated_query, query)\n",
    "\n",
    "\n",
    "# Generate synthetic database\n",
    "# TODO\n",
    "valid_pred, keyword_score, identifier_score = compute_metrics(generated_query, query, test_data['db_id'], synth_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: False\n",
      "valid_pred: True\n",
      "keyword_score: 1.0\n",
      "identifier_score: 0.0\n",
      "Equivalence score: 0.5\n",
      "Explanation: I would assign an equivalence score of 0.5 to these two SQL queries.\n",
      "\n",
      "The primary reason for this score is that the generated query (Query 2) has a different table name (`department_heads`) compared to Query 1 (`head`). This difference in table name means that Query 2 will return different data than Query 1, as it only considers rows in `department_heads` with an age greater than 56.\n",
      "\n",
      "However, there are minor similarities between the two queries. Both use the same filter condition (`age > 56`) and both select the count of rows that satisfy this condition. Additionally, the queries have a similar structure, using the `SELECT COUNT(*)` syntax to count the number of rows meeting the specified criteria.\n",
      "\n",
      "Overall, while Query 2 is not fully logically equivalent to Query 1 due to the difference in table name, it shares some similarities in its logic and structure, which warrants a score of 0.5.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correct: {correct}\")\n",
    "print(f\"valid_pred: {valid_pred}\")\n",
    "print(f\"keyword_score: {keyword_score}\")\n",
    "print(f\"identifier_score: {identifier_score}\")\n",
    "print(f\"Equivalence score: {equivalence_score}\")\n",
    "print(f\"Explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = load_dataset(\"xlangai/spider\")\n",
    "db_schema = pd.DataFrame(load_dataset('richardr1126/spider-schema', split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetix data\n",
    "try:\n",
    "    sql_databases = generate_synthetic_db(db_schema)\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ERROR - i=95 / db : world_1 / table : sqlite_sequence\n",
      "object name reserved for internal use: sqlite_sequence \n",
      "\n",
      "CREATE TABLE \"sqlite_sequence\" (\n",
      "    \"name\" TEXT,\n",
      "    \"seq\" TEXT\n",
      ")\n",
      "INSERT INTO \"sqlite_sequence\" (\"name\", \"seq\") VALUES (?, ?) \n",
      "\n",
      ">>> ERROR - i=111 / db : soccer_1 / table : sqlite_sequence\n",
      "object name reserved for internal use: sqlite_sequence \n",
      "\n",
      "CREATE TABLE \"sqlite_sequence\" (\n",
      "    \"name\" TEXT,\n",
      "    \"seq\" TEXT\n",
      ")\n",
      "INSERT INTO \"sqlite_sequence\" (\"name\", \"seq\") VALUES (?, ?) \n",
      "\n",
      ">>> ERROR - i=128 / db : store_1 / table : sqlite_sequence\n",
      "object name reserved for internal use: sqlite_sequence \n",
      "\n",
      "CREATE TABLE \"sqlite_sequence\" (\n",
      "    \"name\" TEXT,\n",
      "    \"seq\" TEXT\n",
      ")\n",
      "INSERT INTO \"sqlite_sequence\" (\"name\", \"seq\") VALUES (?, ?) \n",
      "\n",
      "\n",
      "--------------\n",
      "\n",
      "ERROR (db :department_management) : no such column: age\n",
      "\n",
      "Error for query:\n",
      "query: SELECT count(*) FROM head WHERE age  >  56\n",
      "gen query: SELECT COUNT(*) FROM management WHERE age > 56\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.666699996667, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: T2.age\n",
      "\n",
      "Error for query:\n",
      "query: SELECT name ,  born_state ,  age FROM head ORDER BY age\n",
      "gen query: SELECT T1.Name , T3.born_state , T2.age FROM department AS T1 JOIN management AS T2 ON T1.Department_ID = T2.head_ID JOIN head AS T3 ON T2.head_ID = T3.head_ID ORDER BY T2.age\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.1500177852711345, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: CreationYear\n",
      "\n",
      "Error for query:\n",
      "query: SELECT creation ,  name ,  budget_in_billions FROM department\n",
      "gen query: SELECT CreationYear , Name , Budget FROM Department\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.6428673466472387, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: Budget\n",
      "\n",
      "Error for query:\n",
      "query: SELECT max(budget_in_billions) ,  min(budget_in_billions) FROM department\n",
      "gen query: SELECT max(Budget) , min(Budget) FROM department\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.500019999200032, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: employees\n",
      "\n",
      "Error for query:\n",
      "query: SELECT avg(num_employees) FROM department WHERE ranking BETWEEN 10 AND 15\n",
      "gen query: SELECT avg(employees) FROM department WHERE rank BETWEEN 10 AND 15\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.6000346633070005, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such table: employee\n",
      "\n",
      "Error for query:\n",
      "query: SELECT name FROM head WHERE born_state != 'California'\n",
      "gen query: SELECT T1.name FROM employee AS T1 JOIN department AS T2 ON T1.head_id = T2.head_id WHERE T2.state != 'California'\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.22226531629867632, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such table: employee\n",
      "\n",
      "Error for query:\n",
      "query: SELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T3.born_state  =  'Alabama'\n",
      "gen query: SELECT DISTINCT T1.create_year FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.head_id JOIN employee AS T3 ON T2.head_id = T3.id WHERE T3.state = 'ALABAMA'\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.6605063209014163, 'result_metric': 0}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT born_state FROM head GROUP BY born_state HAVING count(*)  >=  3\n",
      "gen query: SELECT T2.name FROM management AS T1 JOIN department AS T2 ON T1.head_id = T2.department_id GROUP BY T2.name HAVING COUNT(T1.head_id) >= 3\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.1667239847242572, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: Creation_Date\n",
      "\n",
      "Error for query:\n",
      "query: SELECT creation FROM department GROUP BY creation ORDER BY count(*) DESC LIMIT 1\n",
      "gen query: SELECT YEAR(Creation_Date) , COUNT(*) FROM DEPARTMENT GROUP BY YEAR(Creation_Date) ORDER BY COUNT(*) DESC LIMIT 1;\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.43754322551681807, 'result_metric': 0}\n",
      "ERROR (db :department_management) : near \"TEMPACTING_VALUE\": syntax error\n",
      "\n",
      "Error for query:\n",
      "query: SELECT T1.name ,  T1.num_employees FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id WHERE T2.temporary_acting  =  'Yes'\n",
      "gen query: SELECT T1.Name ,  COUNT(T2.EMPLOYEE_ID) FROM department AS T1 JOIN management AS T2 ON T1.Department_ID = T2.head_ID WHERE T2 TEMPACTING_VALUE = 'YES' GROUP BY T1.Name\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 0.8571469386588955, 'identifiers_score': 0.623821938335575, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: status\n",
      "\n",
      "Error for query:\n",
      "query: SELECT count(DISTINCT temporary_acting) FROM management\n",
      "gen query: SELECT count(*) FROM management WHERE status = 'Acting'\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 0.8000079996800128, 'identifiers_score': 0.33337777481501235, 'result_metric': 0}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT count(*) FROM department WHERE department_id NOT IN (SELECT department_id FROM management);\n",
      "gen query: SELECT COUNT(DISTINCT t1.department_id) FROM department AS t1 JOIN management AS t2 ON t1.department_id = t2.head_id WHERE t2.head_id NOT IN (SELECT head_id FROM department)\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.2500687432819179, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such table: person\n",
      "\n",
      "Error for query:\n",
      "query: SELECT DISTINCT T1.age FROM management AS T2 JOIN head AS T1 ON T1.head_id  =  T2.head_id WHERE T2.temporary_acting  =  'Yes'\n",
      "gen query: SELECT DISTINCT T2.age FROM management AS T1 JOIN person AS T2 ON T1.head_id = T2.head_id WHERE T1.is_acting = 1\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.5777938511849673, 'result_metric': 0}\n",
      "ERROR (db :department_management) : no such column: T1.state\n",
      "\n",
      "Error for query:\n",
      "query: SELECT T3.born_state FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T1.name  =  'Treasury' INTERSECT SELECT T3.born_state FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T1.name  =  'Homeland Security'\n",
      "gen query: SELECT T1.state FROM management AS T1 JOIN department AS T2 ON T1.head_id = T2.head_id WHERE T2.name = 'Treasury' AND T1.born_state = ( SELECT born_state FROM management WHERE head_id IN ( SELECT head_id FROM department WHERE name = 'Homeland Security' ) );\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.2444842928586067, 'result_metric': 0}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT T1.department_id ,  T1.name ,  count(*) FROM management AS T2 JOIN department AS T1 ON T1.department_id  =  T2.department_id GROUP BY T1.department_id HAVING count(*)  >  1\n",
      "gen query: SELECT department_id ,  name ,  count(*) FROM department GROUP BY department_id HAVING COUNT(*)  >  1\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.3669169352190621, 'result_metric': 1}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT head_id ,  name FROM head WHERE name LIKE '%Ha%'\n",
      "gen query: SELECT head_id ,  name FROM head WHERE name LIKE '%Ha%'\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 0.9333359998933376, 'result_metric': 1}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT count(*) FROM farm\n",
      "gen query: SELECT count(*) FROM farm\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 1.0, 'result_metric': 1}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT count(*) FROM farm\n",
      "gen query: SELECT COUNT(*) FROM farm\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 1.0, 'identifiers_score': 1.0, 'result_metric': 1}\n",
      "ERROR (db :farm) : no such column: farm_competition\n",
      "\n",
      "Error for query:\n",
      "query: SELECT Total_Horses FROM farm ORDER BY Total_Horses ASC\n",
      "gen query: SELECT COUNT(*) FROM farm WHERE farm_competition = 'Horses' ORDER BY COUNT(*) ASC;\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 0.8571469386588955, 'identifiers_score': 0.13338577375443253, 'result_metric': 0}\n",
      "\n",
      "Error for query:\n",
      "query: SELECT Total_Horses FROM farm ORDER BY Total_Horses ASC\n",
      "gen query: SELECT COUNT(*) FROM farm GROUP BY farm_id ORDER BY COUNT(*) ASC;\n",
      "metrics:\n",
      "{'valid_pred': True, 'keyword_score': 0.8571469386588955, 'identifiers_score': 0.13339132884705443, 'result_metric': 0}\n",
      "\n",
      "\n",
      "Proportion of errs\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics on synth data\n",
    "\n",
    "import itertools\n",
    "\n",
    "k = 20\n",
    "print(f\"\\n--------------\\n\")\n",
    "nb_err = 0\n",
    "for data in itertools.islice(ds['train'], k):\n",
    "    question = data['question']\n",
    "    query1 = data['query']\n",
    "    db_id = data['db_id']\n",
    "    schema = db_schema[db_schema['db_id'] == db_id]\n",
    "\n",
    "    generated_query = llm.generate_query(question, schema=schema)\n",
    "    metrics = compute_metrics(query1, generated_query, db_id, sql_databases)\n",
    "    errs = dict()\n",
    "    if metrics['result_metric'] == 0:\n",
    "        nb_err += 1\n",
    "        errs['query'] = query1\n",
    "        errs['gen_query'] = generated_query\n",
    "    print(f\"\\nError for query:\")\n",
    "    print(f\"query: {query1}\")\n",
    "    print(f\"gen query: {generated_query}\")\n",
    "    print(f\"metrics:\")\n",
    "    print(metrics)\n",
    "\n",
    "print(f\"\\n\\nProportion of errs\")\n",
    "print(nb_err/k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a retrieval tool for generating the SQL Query. The goal is to retrieve relevant data: schemas of relevant databases\n",
    "Indeed, for the moment we put some database schemas in the prompt context, however this may be illegal as being information leak. In a real case production text2SQL tool, we may not have a table schema linked to a user query. Moreover, a query may need to join table, to retrieve K database schema may be efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets retrieve relevant schemas\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
